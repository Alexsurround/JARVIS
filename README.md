# ðŸ¤– JARVIS - Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ AI ÐÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚

> Ð“Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ð¹ AI-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð² ÑÑ‚Ð¸Ð»Ðµ JARVIS Ð¸Ð· Iron Man, Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‰Ð¸Ð¹ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð½Ð° Ð²Ð°ÑˆÐµÐ¼ ÑÐµÑ€Ð²ÐµÑ€Ðµ

## ðŸ“‹ Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ

- [Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ AI](#Ñ‡Ñ‚Ð¾-Ñ‚Ð°ÐºÐ¾Ðµ-Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹-ai)
- [ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° JARVIS](#Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°-jarvis)
- [Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ](#ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ðµ-Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ)
- [Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°](#ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°)
- [ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°](#Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°)
- [Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ](#Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ)
- [ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹](#Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹)
- [FAQ](#faq)

---

## ðŸ§  Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ AI

**Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ AI** - ÑÑ‚Ð¾ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð½Ð° Ð²Ð°ÑˆÐµÐ¼ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð±ÐµÐ· Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð¾Ð±Ð»Ð°ÐºÐ¾.

### ÐŸÑ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð°:
- âœ… **ÐŸÑ€Ð¸Ð²Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ** - Ð²ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ÑÑ‚Ð°ÑŽÑ‚ÑÑ Ð½Ð° Ð²Ð°ÑˆÐµÐ¼ ÑÐµÑ€Ð²ÐµÑ€Ðµ
- âœ… **ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ** - Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼
- âœ… **ÐÐµÑ‚ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸** Ð¾Ñ‚ Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ð° Ð¸ Ð²Ð½ÐµÑˆÐ½Ð¸Ñ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
- âœ… **Ð‘ÐµÐ· Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹** - Ð½ÐµÑ‚ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð² Ð½Ð° Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹
- âœ… **Ð‘ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ð¾** - Ð¿Ð¾ÑÐ»Ðµ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸

### ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ / Ð¢ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð²Ð²Ð¾Ð´              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speech-to-Text (Whisper)               â”‚
â”‚  Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Large Language Model (Llama/Mistral)   â”‚
â”‚  "ÐœÐ¾Ð·Ð³" Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text-to-Speech (Piper/Coqui)           â”‚
â”‚  Ð¡Ð¸Ð½Ñ‚ÐµÐ· Ñ€ÐµÑ‡Ð¸                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ð”Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ / Ð¢ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ—ï¸ ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° JARVIS

JARVIS ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¼Ð¾Ð´ÑƒÐ»ÐµÐ¹, Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‰Ð¸Ñ… Ð² Docker-ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°Ñ…:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   JARVIS System                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   Ollama     â”‚  â”‚  Open WebUI  â”‚              â”‚
â”‚  â”‚   (LLM)      â”‚â—„â”€â”¤  (Interface) â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â–²                  â–²                       â”‚
â”‚         â”‚                  â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Whisper   â”‚  â”‚    Piper     â”‚               â”‚
â”‚  â”‚    (STT)    â”‚  â”‚    (TTS)     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â–²                  â”‚                       â”‚
â”‚         â”‚                  â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚      Audio Pipeline             â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚     ChromaDB (Vector Store)      â”‚             â”‚
â”‚  â”‚     RAG Ð´Ð»Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ’» Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ

### ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ (Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ):
- **CPU**: 8 ÑÐ´ÐµÑ€
- **RAM**: 16 GB
- **Ð”Ð¸ÑÐº**: 100 GB
- **ÐžÐ¡**: Ubuntu 22.04 / Debian 12
- **Docker**: 24.0+

### Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ðµ (Ð´Ð»Ñ ÐºÐ¾Ð¼Ñ„Ð¾Ñ€Ñ‚Ð½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹):
- **CPU**: 16 ÑÐ´ÐµÑ€
- **RAM**: 32-64 GB
- **Ð”Ð¸ÑÐº**: 200 GB SSD
- **GPU**: NVIDIA RTX 3090/4090 Ð¸Ð»Ð¸ Ð²Ñ‹ÑˆÐµ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾, Ð½Ð¾ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¶ÐµÐ»Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾)
- **VRAM**: 16-24 GB (Ð´Ð»Ñ GPU)

### Ð”Ð»Ñ production:
- **CPU**: 24+ ÑÐ´ÐµÑ€
- **RAM**: 64-128 GB
- **GPU**: NVIDIA A100 / H100
- **VRAM**: 40-80 GB

### Ð Ð°Ð·Ð¼ÐµÑ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹:

| ÐœÐ¾Ð´ÐµÐ»ÑŒ | Ð Ð°Ð·Ð¼ÐµÑ€ | RAM | GPU VRAM | Ð¡ÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ |
|--------|--------|-----|----------|----------|
| Llama 3.1 8B | 4.7 GB | 16 GB | 6 GB | âš¡âš¡âš¡ |
| Mistral 7B | 4.1 GB | 16 GB | 8 GB | âš¡âš¡âš¡ |
| Llama 3.1 70B | 40 GB | 64 GB | 40 GB | âš¡âš¡ |
| Mixtral 8x7B | 26 GB | 32 GB | 24 GB | âš¡âš¡ |

---

## ðŸš€ Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°

### Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1: Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° (Docker Compose)

#### 1. ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹

```bash
# ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
sudo apt update && sudo apt upgrade -y

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð² Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ docker
sudo usermod -aG docker $USER
newgrp docker

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Docker Compose
sudo apt install docker-compose-plugin -y
```

#### 2. Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
mkdir -p ~/jarvis && cd ~/jarvis

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ docker-compose.yml
cat > docker-compose.yml <<EOF
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: jarvis-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    # Ð Ð°ÑÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð´Ð»Ñ GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: jarvis-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=your-secret-key-change-this
      - ENABLE_RAG_WEB_SEARCH=true
      - ENABLE_IMAGE_GENERATION=false
    volumes:
      - open_webui_data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped

  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: jarvis-whisper
    ports:
      - "9000:9000"
    environment:
      - ASR_MODEL=base
      - ASR_ENGINE=openai_whisper
    restart: unless-stopped

  piper:
    image: rhasspy/wyoming-piper:latest
    container_name: jarvis-piper
    ports:
      - "10200:10200"
    command: --voice en_GB-alan-medium
    restart: unless-stopped

  chromadb:
    image: chromadb/chroma:latest
    container_name: jarvis-chromadb
    volumes:
      - chromadb_data:/chroma/chroma
    ports:
      - "8000:8000"
    environment:
      - IS_PERSISTENT=TRUE
    restart: unless-stopped

volumes:
  ollama_data:
  open_webui_data:
  chromadb_data:

networks:
  default:
    name: jarvis-network
EOF
```

#### 3. Ð—Ð°Ð¿ÑƒÑÐº ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹

```bash
# Ð—Ð°Ð¿ÑƒÑÐº Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
docker compose up -d

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ñ‚ÑƒÑÐ°
docker compose ps

# ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð»Ð¾Ð³Ð¾Ð²
docker compose logs -f
```

#### 4. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹

```bash
# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° LLM Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð²Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¾Ð´Ð½Ñƒ)
docker exec -it jarvis-ollama ollama pull llama3.1:8b      # Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»Ð°)
# docker exec -it jarvis-ollama ollama pull llama3.1:70b   # Ð£Ð¼Ð½Ð°Ñ (Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¼Ð½Ð¾Ð³Ð¾ RAM)
# docker exec -it jarvis-ollama ollama pull mistral:7b     # ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð°

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
docker exec -it jarvis-ollama ollama list
```

#### 5. ÐŸÐµÑ€Ð²Ñ‹Ð¹ Ð·Ð°Ð¿ÑƒÑÐº

ÐžÑ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð¸ Ð¿ÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð½Ð°:
```
http://your-server-ip:3000
```

Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ð°ÐºÐºÐ°ÑƒÐ½Ñ‚ Ð¸ Ð½Ð°Ñ‡Ð½Ð¸Ñ‚Ðµ Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ñ JARVIS!

---

### Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2: Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½Ð° Proxmox/ESXi

#### Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ VM:

**Proxmox:**
```bash
# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ VM Ñ‡ÐµÑ€ÐµÐ· Web UI Ð¸Ð»Ð¸ CLI
qm create 100 \
  --name jarvis \
  --memory 32768 \
  --cores 16 \
  --net0 virtio,bridge=vmbr0 \
  --scsi0 local-lvm:200
```

**ESXi:**
- Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ VM Ñ‡ÐµÑ€ÐµÐ· vSphere Client
- Ubuntu 22.04 LTS
- 32 GB RAM, 16 vCPU, 200 GB Disk

#### GPU Passthrough (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾):

**Proxmox:**
```bash
# Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ IOMMU
nano /etc/default/grub
# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ: intel_iommu=on (Ð¸Ð»Ð¸ amd_iommu=on)

update-grub
reboot

# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ GPU Ðº VM
qm set 100 -hostpci0 01:00,pcie=1
```

Ð”Ð°Ð»ÐµÐµ ÑÐ»ÐµÐ´ÑƒÐ¹Ñ‚Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼ Ð¸Ð· Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð° 1.

---

### Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 3: ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ollama)

```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Ð—Ð°Ð¿ÑƒÑÐº Ð¼Ð¾Ð´ÐµÐ»Ð¸
ollama run llama3.1:8b

# Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
ollama run llama3.1:8b "Hello, I am JARVIS"
```

---

## âš™ï¸ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°

### 1. ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ JARVIS Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸

Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ñ„Ð°Ð¹Ð» Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð¼:

```bash
cat > ~/jarvis/jarvis-prompt.txt <<EOF
You are JARVIS (Just A Rather Very Intelligent System), an advanced AI assistant 
created in the style of Tony Stark's AI from Iron Man.

Personality traits:
- Sophisticated and articulate with a British accent personality
- Professional but with subtle dry humor
- Highly efficient and proactive
- Address user as "Sir" or by their name
- Provide concise, accurate information
- Anticipate needs when possible

Capabilities:
- Answer questions and provide information
- Control smart home devices (when integrated)
- Manage schedules and reminders
- Perform calculations and data analysis
- Access and summarize documents

Always maintain professionalism while being helpful and personable.
EOF
```

### 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Open WebUI

1. ÐžÑ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ http://your-server-ip:3000
2. ÐŸÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð² **Settings** â†’ **Models**
3. Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (llama3.1:8b)
4. Ð’ **System Prompt** Ð²ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ `jarvis-prompt.txt`
5. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹Ñ‚Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹:
   - **Temperature**: 0.7 (Ð±Ð°Ð»Ð°Ð½Ñ ÐºÑ€ÐµÐ°Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸)
   - **Top P**: 0.9
   - **Max Tokens**: 2048

### 3. Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² (RAG)

```bash
# Ð§ÐµÑ€ÐµÐ· Web UI:
# 1. ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ Ð½Ð° Ð¸ÐºÐ¾Ð½ÐºÑƒ "+" Ð² Ñ‡Ð°Ñ‚Ðµ
# 2. Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ "Upload Files"
# 3. Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ PDF/TXT/MD Ñ„Ð°Ð¹Ð»Ñ‹
# 4. JARVIS Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ð¾ ÑÑ‚Ð¸Ð¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼
```

### 4. Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ Home Assistant (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)

```bash
# Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ Ð² docker-compose.yml:
  homeassistant:
    image: homeassistant/home-assistant:latest
    container_name: jarvis-homeassistant
    ports:
      - "8123:8123"
    volumes:
      - ./homeassistant:/config
    restart: unless-stopped
```

### 5. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ð³Ð¾ Ð²Ð²Ð¾Ð´Ð°

Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Python ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸:

```bash
cat > ~/jarvis/voice_client.py <<EOF
import requests
import pyaudio
import wave

WHISPER_URL = "http://localhost:9000/asr"
OLLAMA_URL = "http://localhost:11434/api/generate"
PIPER_URL = "http://localhost:10200/api/tts"

def record_audio(duration=5):
    """Ð—Ð°Ð¿Ð¸ÑÑŒ Ð°ÑƒÐ´Ð¸Ð¾ Ñ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°"""
    # Ð’Ð°Ñˆ ÐºÐ¾Ð´ Ð·Ð°Ð¿Ð¸ÑÐ¸
    pass

def transcribe(audio_file):
    """Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ Ñ‡ÐµÑ€ÐµÐ· Whisper"""
    files = {'audio_file': open(audio_file, 'rb')}
    response = requests.post(WHISPER_URL, files=files)
    return response.json()['text']

def ask_jarvis(text):
    """ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ðº Ollama"""
    data = {
        "model": "llama3.1:8b",
        "prompt": f"As JARVIS: {text}",
        "stream": False
    }
    response = requests.post(OLLAMA_URL, json=data)
    return response.json()['response']

def speak(text):
    """Ð¡Ð¸Ð½Ñ‚ÐµÐ· Ñ€ÐµÑ‡Ð¸ Ñ‡ÐµÑ€ÐµÐ· Piper"""
    response = requests.post(PIPER_URL, json={"text": text})
    # Ð’Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾
    pass

# Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ»
while True:
    print("Listening...")
    audio = record_audio()
    text = transcribe(audio)
    
    if "jarvis" in text.lower():
        print(f"You: {text}")
        response = ask_jarvis(text)
        print(f"JARVIS: {response}")
        speak(response)
EOF
```

---

## ðŸŽ¯ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ

### ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹

#### Ð§ÐµÑ€ÐµÐ· Web Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ:
```
http://your-server-ip:3000
```

#### Ð§ÐµÑ€ÐµÐ· API:

```bash
# ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "What is the weather like today?",
  "stream": false
}'

# Ð¡ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð¼
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.1:8b",
  "messages": [
    {
      "role": "system",
      "content": "You are JARVIS, a helpful AI assistant."
    },
    {
      "role": "user",
      "content": "Good morning JARVIS"
    }
  ]
}'
```

#### Ð§ÐµÑ€ÐµÐ· CLI:

```bash
# Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼
docker exec -it jarvis-ollama ollama run llama3.1:8b

# ÐžÐ´Ð¸Ð½Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
docker exec -it jarvis-ollama ollama run llama3.1:8b "What time is it?"
```

### ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ

**ÐžÐ±Ñ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹:**
```
User: JARVIS, what's the capital of France?
JARVIS: The capital of France is Paris, sir.
```

**ÐÐ½Ð°Ð»Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²:**
```
User: Summarize the uploaded contract
JARVIS: Based on the document, the key points are...
```

**Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ:**
```
User: Calculate 15% tip on $85.50
JARVIS: A 15% tip on $85.50 would be $12.83, bringing the total to $98.33, sir.
```

---

## ðŸ”„ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹

### ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¹

```bash
# Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
docker exec -it jarvis-ollama ollama list

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð¾Ð²Ñ‹Ñ… Ð²ÐµÑ€ÑÐ¸Ð¹ Ð½Ð° ollama.com/library
```

### ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸

```bash
# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð½Ð¾Ð²Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸
docker exec -it jarvis-ollama ollama pull llama3.1:latest

# Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸
docker exec -it jarvis-ollama ollama rm llama3.1:old
```

### ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ cron Ð·Ð°Ð´Ð°Ñ‡Ð¸
crontab -e

# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð¾ÐºÑƒ (Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ°Ð¶Ð´Ð¾Ðµ Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÑŒÐµ Ð² 2:00)
0 2 * * 0 docker exec jarvis-ollama ollama pull llama3.1:latest
```

### ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Docker ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð¾Ð²

```bash
cd ~/jarvis

# ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
docker compose down

# ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²
docker compose pull

# Ð—Ð°Ð¿ÑƒÑÐº Ñ Ð½Ð¾Ð²Ñ‹Ð¼Ð¸ Ð²ÐµÑ€ÑÐ¸ÑÐ¼Ð¸
docker compose up -d
```

---

## ðŸ“Š ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ

### ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð»Ð¾Ð³Ð¾Ð²

```bash
# Ð’ÑÐµ ÑÐµÑ€Ð²Ð¸ÑÑ‹
docker compose logs -f

# ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ ÑÐµÑ€Ð²Ð¸Ñ
docker compose logs -f ollama
docker compose logs -f open-webui
```

### ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²

```bash
# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°Ð¼Ð¸
docker stats

# Ð”Ð¸ÑÐºÐ¾Ð²Ð¾Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾
docker system df
```

### Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ðµ ÐºÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð±ÑÐºÐ°Ð¿Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…
cd ~/jarvis
docker compose down

# Ð‘ÑÐºÐ°Ð¿ volumes
sudo tar -czf jarvis-backup-$(date +%Y%m%d).tar.gz \
  /var/lib/docker/volumes/jarvis_*

docker compose up -d
```

### Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ

```bash
# ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
docker compose down

# Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð· Ð±ÑÐºÐ°Ð¿Ð°
sudo tar -xzf jarvis-backup-20240101.tar.gz -C /

# Ð—Ð°Ð¿ÑƒÑÐº ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
docker compose up -d
```

---

## ðŸ”§ Troubleshooting

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: ÐœÐ¾Ð´ÐµÐ»ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
- Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ñ‚Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ 8B Ð²Ð¼ÐµÑÑ‚Ð¾ 70B)
- Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ GPU passthrough
- Ð£Ð²ÐµÐ»Ð¸Ñ‡ÑŒÑ‚Ðµ RAM VM
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Q4_K_M)

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: "Out of memory"

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸
docker stats

# Ð£Ð²ÐµÐ»Ð¸Ñ‡ÑŒÑ‚Ðµ RAM VM Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¼ÐµÐ½ÑŒÑˆÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
docker exec -it jarvis-ollama ollama pull llama3.1:8b
```

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: ÐÐµ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒÑÑ Ðº Web UI

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾Ñ€Ñ‚Ð¾Ð²
sudo netstat -tulpn | grep 3000

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° firewall
sudo ufw allow 3000

# ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐº ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°
docker restart jarvis-webui
```

### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: GPU Ð½Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ÑÑ

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

---

## â“ FAQ

### ÐœÐ¾Ð¶Ð½Ð¾ Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð±ÐµÐ· GPU?

Ð”Ð°! ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð±ÑƒÐ´ÑƒÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð½Ð° CPU, Ð½Ð¾ Ð¼ÐµÐ´Ð»ÐµÐ½Ð½ÐµÐµ. Ð”Ð»Ñ ÐºÐ¾Ð¼Ñ„Ð¾Ñ€Ñ‚Ð½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 7B-8B.

### Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ð¾Ð¸Ñ‚ Ð·Ð°Ð¿ÑƒÑÐº?

ÐŸÐ¾ÑÐ»Ðµ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ - Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ð¾! ÐÑƒÐ¶Ð½Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ»ÐµÐºÑ‚Ñ€Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð»Ñ ÑÐµÑ€Ð²ÐµÑ€Ð°.

### ÐžÐ±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð° Ð¼Ð¾Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…?

ÐÐµÑ‚, Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ inference (Ð²Ñ‹Ð²Ð¾Ð´). Ð”Ð»Ñ "Ð¿Ð°Ð¼ÑÑ‚Ð¸" Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ RAG - Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€ÑƒÑŽÑ‚ÑÑ, Ð½Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ.

### ÐœÐ¾Ð¶Ð½Ð¾ Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹?

Ð”Ð°! Ollama Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½ÑƒÑŽ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹.

### ÐšÐ°Ðº Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°?

ÐœÐ¾Ð´ÐµÐ»Ð¸ Llama 3.1 Ð¸ Mistral ÑƒÐ¶Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ñ€ÑƒÑÑÐºÐ¸Ð¹. Ð”Ð»Ñ Ð³Ð¾Ð»Ð¾ÑÐ° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚Ðµ Ñ€ÑƒÑÑÐºÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Piper:
```bash
docker exec -it jarvis-piper --voice ru_RU-...
```

### Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ Ð»Ð¸ ÑÑ‚Ð¾?

Ð”Ð°! Ð’ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ÑÑ‚Ð°ÑŽÑ‚ÑÑ Ð½Ð° Ð²Ð°ÑˆÐµÐ¼ ÑÐµÑ€Ð²ÐµÑ€Ðµ, Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ð² Ð¾Ð±Ð»Ð°ÐºÐ¾.

### Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¼ÐµÑÑ‚Ð° Ð½Ð° Ð´Ð¸ÑÐºÐµ Ð½ÑƒÐ¶Ð½Ð¾?

- Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð°: 20 GB
- ÐžÐ´Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ 8B: ~5 GB
- Open WebUI + Ð´Ð°Ð½Ð½Ñ‹Ðµ: ~10 GB
- Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¸ Ð»Ð¾Ð³Ð¸: ~20-50 GB
- **Ð˜Ñ‚Ð¾Ð³Ð¾:** 100-200 GB Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ

---

## ðŸ“š ÐŸÐ¾Ð»ÐµÐ·Ð½Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Open WebUI](https://github.com/open-webui/open-webui)
- [Available Models](https://ollama.com/library)
- [Whisper by OpenAI](https://github.com/openai/whisper)
- [Piper TTS](https://github.com/rhasspy/piper)

---

## ðŸ¤ ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°

Ð•ÑÐ»Ð¸ Ñƒ Ð²Ð°Ñ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹:
1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ [FAQ](#faq)
2. Ð˜Ð·ÑƒÑ‡Ð¸Ñ‚Ðµ Ð»Ð¾Ð³Ð¸: `docker compose logs -f`
3. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ [Issues Ð½Ð° GitHub](https://github.com/ollama/ollama/issues)

---

## ðŸ“ Ð›Ð¸Ñ†ÐµÐ½Ð·Ð¸Ñ

Ð­Ñ‚Ð¾Ñ‚ Ð¿Ñ€Ð¾ÐµÐºÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¾Ðµ ÐŸÐž:
- Ollama - MIT License
- Open WebUI - MIT License
- Llama 3.1 - Meta's Community License

---

**Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ Ñ â¤ï¸ Ð´Ð»Ñ Ð»ÑŽÐ±Ð¸Ñ‚ÐµÐ»ÐµÐ¹ Iron Man Ð¸ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ AI**

*"Sometimes you gotta run before you can walk." - Tony Stark*
